{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7768267,
          "sourceType": "datasetVersion",
          "datasetId": 4544241
        }
      ],
      "dockerImageVersionId": 30512,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The task consist to classify the semantic of tweets. To deal with a deep understanding of natural language I'll try directly to handle the task with the transformer achitecture.\n"
      ],
      "metadata": {
        "id": "c5u6ak0HQqZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I'll use the HF library"
      ],
      "metadata": {
        "id": "cazbPWtXRWxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --disable-pip-version-check \\\n",
        "    torch \\\n",
        "    torchdata --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers \\\n",
        "    datasets \\\n",
        "    evaluate --quiet\n",
        "\n",
        "%pip install accelerate -U --quiet"
      ],
      "metadata": {
        "id": "FzmtaVR6IE3a",
        "outputId": "11e14ee5-bb2b-453d-96e2-0b6a21732503",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:34:53.832608Z",
          "iopub.execute_input": "2024-03-05T16:34:53.833077Z",
          "iopub.status.idle": "2024-03-05T16:35:29.169337Z",
          "shell.execute_reply.started": "2024-03-05T16:34:53.833050Z",
          "shell.execute_reply": "2024-03-05T16:35:29.168137Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import  AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "G2I2tgzDIY1g",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:42:50.903169Z",
          "iopub.execute_input": "2024-03-05T16:42:50.903557Z",
          "iopub.status.idle": "2024-03-05T16:43:05.346320Z",
          "shell.execute_reply.started": "2024-03-05T16:42:50.903525Z",
          "shell.execute_reply": "2024-03-05T16:43:05.345358Z"
        },
        "trusted": true,
        "outputId": "427f31b2-6349-4e57-b21c-c8b31ebac508"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def from_txt_to_pd(files_dir):\n",
        "    categories = ['neg', 'pos']\n",
        "\n",
        "    df = pd.DataFrame(columns=['text','sentiment'])\n",
        "\n",
        "    for category in categories:\n",
        "        path = os.path.join(files_dir, category)\n",
        "        for txt in os.listdir(path):\n",
        "            txt_path = os.path.join(path, txt)\n",
        "            if os.path.getsize(txt_path) > 0:\n",
        "                label = categories.index(category)\n",
        "                file = open(txt_path, \"r\")\n",
        "                text = file.read()\n",
        "                file.close()\n",
        "                new = [text, label]\n",
        "                # New data as pandas.DataFrame\n",
        "                new = pd.DataFrame(columns=df.columns, data=[new])\n",
        "                df = pd.concat([df, new], ignore_index=True)\n",
        "    return df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T16:37:51.936460Z",
          "iopub.execute_input": "2024-03-05T16:37:51.936830Z",
          "iopub.status.idle": "2024-03-05T16:37:51.944985Z",
          "shell.execute_reply.started": "2024-03-05T16:37:51.936800Z",
          "shell.execute_reply": "2024-03-05T16:37:51.943937Z"
        },
        "trusted": true,
        "id": "x4oDSMxvEKsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_dir = '/kaggle/input/movie-reviews/aclImdb/train/'\n",
        "train_df = from_txt_to_pd(file_dir)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T16:37:56.697823Z",
          "iopub.execute_input": "2024-03-05T16:37:56.698655Z",
          "iopub.status.idle": "2024-03-05T16:39:41.104847Z",
          "shell.execute_reply.started": "2024-03-05T16:37:56.698623Z",
          "shell.execute_reply": "2024-03-05T16:39:41.104039Z"
        },
        "trusted": true,
        "id": "HP3NIo8lEKsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T16:39:41.106446Z",
          "iopub.execute_input": "2024-03-05T16:39:41.106724Z",
          "iopub.status.idle": "2024-03-05T16:39:41.122876Z",
          "shell.execute_reply.started": "2024-03-05T16:39:41.106699Z",
          "shell.execute_reply": "2024-03-05T16:39:41.121914Z"
        },
        "trusted": true,
        "id": "iOFyv8hhEKsi",
        "outputId": "9cdb4310-c1c1-4180-d147-55be322e3f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                text sentiment\n0  This film is the worst film, but it ranks very...         0\n1  I should never have started this film, and sto...         0\n2  I'm here again in your local shopping mall (of...         0\n3  Black and White film. Good photography. Believ...         0\n4  from the start of this movie you soon become a...         0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>This film is the worst film, but it ranks very...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I should never have started this film, and sto...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I'm here again in your local shopping mall (of...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Black and White film. Good photography. Believ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>from the start of this movie you soon become a...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.groupby(['sentiment']).count()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T16:39:57.624752Z",
          "iopub.execute_input": "2024-03-05T16:39:57.625100Z",
          "iopub.status.idle": "2024-03-05T16:39:57.652999Z",
          "shell.execute_reply.started": "2024-03-05T16:39:57.625074Z",
          "shell.execute_reply": "2024-03-05T16:39:57.652093Z"
        },
        "trusted": true,
        "id": "U5wdQqHBEKsj",
        "outputId": "a62fed50-4027-473b-e6cc-4e858b0b30d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "            text\nsentiment       \n0          12500\n1          12500",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>sentiment</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_dir = '/kaggle/input/movie-reviews/aclImdb/test/'\n",
        "test_df = from_txt_to_pd(file_dir)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T16:40:05.844228Z",
          "iopub.execute_input": "2024-03-05T16:40:05.844633Z",
          "iopub.status.idle": "2024-03-05T16:41:56.835601Z",
          "shell.execute_reply.started": "2024-03-05T16:40:05.844599Z",
          "shell.execute_reply": "2024-03-05T16:41:56.834750Z"
        },
        "trusted": true,
        "id": "gY3-hU-qEKsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check if it's running on the GPU"
      ],
      "metadata": {
        "id": "iGYjGjivoKuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "zdUhtLCWd8M4",
        "outputId": "4168579f-dd92-43dc-8ca8-11a017d86a22",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:43:12.955051Z",
          "iopub.execute_input": "2024-03-05T16:43:12.956098Z",
          "iopub.status.idle": "2024-03-05T16:43:12.991070Z",
          "shell.execute_reply.started": "2024-03-05T16:43:12.956065Z",
          "shell.execute_reply": "2024-03-05T16:43:12.990104Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "There are 1 GPU(s) available.\nDevice name: Tesla P100-PCIE-16GB\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After I made some tests, I wrote a two functions to handle the data cleaning, by removing part of the text that cannot be handle well by the tokenizer"
      ],
      "metadata": {
        "id": "V-HrfIwcEKsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_clean(text):\n",
        "\n",
        "    ### Light\n",
        "    #text = text.lower() # lowercase everything\n",
        "    text = text.encode('ascii', 'ignore').decode()  # remove unicode characters\n",
        "    text = re.sub(r'https*\\S+', ' ', text) # remove links\n",
        "    text = re.sub(r'http*\\S+', ' ', text)\n",
        "    # cleaning up text\n",
        "    #text = re.sub(r'%20', '', text)\n",
        "    text = text.replace(\"%20\", \" \" )\n",
        "    text = re.sub(r'\\'\\w+', '', text)\n",
        "    text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    text = re.sub(r'\\s[^\\w\\s]\\s', '', text)\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "EkJlmf1Bj2Lx",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:46:49.910387Z",
          "iopub.execute_input": "2024-03-05T16:46:49.911397Z",
          "iopub.status.idle": "2024-03-05T16:46:49.918539Z",
          "shell.execute_reply.started": "2024-03-05T16:46:49.911359Z",
          "shell.execute_reply": "2024-03-05T16:46:49.917633Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_promt(df_to_clean):\n",
        "    df_to_clean['text'] = df_to_clean.text.apply(text_clean)\n",
        "    return df_to_clean"
      ],
      "metadata": {
        "id": "p6xx8W74ePnx",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:46:55.853899Z",
          "iopub.execute_input": "2024-03-05T16:46:55.854577Z",
          "iopub.status.idle": "2024-03-05T16:46:55.859601Z",
          "shell.execute_reply.started": "2024-03-05T16:46:55.854533Z",
          "shell.execute_reply": "2024-03-05T16:46:55.858597Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now call the function to clean the text"
      ],
      "metadata": {
        "id": "Ga_e4D10EKsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_cleaned = prepare_promt(train_df)\n",
        "test_df_cleaned = prepare_promt(test_df)"
      ],
      "metadata": {
        "id": "ot5dhICFlQ2f",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:46:58.449333Z",
          "iopub.execute_input": "2024-03-05T16:46:58.449691Z",
          "iopub.status.idle": "2024-03-05T16:47:22.495883Z",
          "shell.execute_reply.started": "2024-03-05T16:46:58.449663Z",
          "shell.execute_reply": "2024-03-05T16:47:22.495078Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.text[554])\n",
        "print(train_df_cleaned.text[554])\n",
        "print(train_df.text[67])\n",
        "print(train_df_cleaned.text[67])"
      ],
      "metadata": {
        "id": "AMEaXTo5t8bo",
        "outputId": "5fafb857-1465-4bd6-f732-8ba6432d8d19",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:47:24.850516Z",
          "iopub.execute_input": "2024-03-05T16:47:24.850857Z",
          "iopub.status.idle": "2024-03-05T16:47:24.856733Z",
          "shell.execute_reply.started": "2024-03-05T16:47:24.850834Z",
          "shell.execute_reply": "2024-03-05T16:47:24.855823Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "i know you read that before, on countless other films no doubt if you reading the comments here, but voodoo academy still stands as the absolute worst film i been able to track down. no doubt the really bad ones aren even available to buy or watch on tv, but even so i feel it fair to qualify that i not just some dumb renter who picked badly. i seen two thirds of the bottom ranked films here on the imdb, and i ticking of the remaining ones with every chance. most of those stand head and shoulders above this... excersize in absolute monotony. i like to rate truly bad films (as in ones with no humour even in how bad they are) by how many people you need to watch it with to make it all the way through. if you can watch it by yourself, it isn that bad. if you can watch it with one single friend... it bad but could be a lot worse. it took of us to make it all the way through voodoo academy. and not even the usually fun moments of watching bad films (spotting the boom mic for example, times in this one if i not mistaken) could take away the dry taste in my mouth. yes i watched it, but i also forced of my friends to watch it with me to achieve that end. i hope and pray we saw the directors cut... to think that there could be a version with more minutes of big eyebrowed lugs with baby oil glands rubbing their torsos just scares the hell out of me. so much of the film centers around this. i do applaud david decoteau for managing to lens this film in two days on a short budget, just as i applaud him for convincing people to pay him to make what is no doubt a celluloid version of some of his fetishes. but it isn a good film. the original shop of horrors was shot in the same length of time for a comparitive amount of money (considering inflation) and was an utter gem. it not an excuse for how bad this baby is. spoilers ahead... it not even worth picking apart the plot holes or cliched know it all hero characters... the pacing of the film... is insane... nothing... is interesting for the length of time decoteau dedicates to the pectoral self massaging. no matter what your alignment or sex... rubbing just cannot sustain that kind of screen time. the acting is cheese... but not overly amatuer... i seen a lot worse in better films... but somehow it the semi competent delivery of some of the worst lines you ever hear in a film, that really grates. rent this if, like me, you have a fascination with the worst of the worst, and only if you going to watch it with a group of people who are prepared to work to get through it. this is no ha ha ha the set wobbled affair. it an endurance test you probably want to skip. i sure there is worse... but i wouldn be surprised if it has decoteau name on it.\ni know you read that before, on countless other films no doubt if you reading the comments here, but voodoo academy still stands as the absolute worst film i been able to track down. no doubt the really bad ones aren even available to buy or watch on tv, but even so i feel it fair to qualify that i not just some dumb renter who picked badly. i seen two thirds of the bottom ranked films here on the imdb, and i ticking of the remaining ones with every chance. most of those stand head and shoulders above this... excersize in absolute monotony. i like to rate truly bad films (as in ones with no humour even in how bad they are) by how many people you need to watch it with to make it all the way through. if you can watch it by yourself, it isn that bad. if you can watch it with one single friend... it bad but could be a lot worse. it took of us to make it all the way through voodoo academy. and not even the usually fun moments of watching bad films (spotting the boom mic for example, times in this one if i not mistaken) could take away the dry taste in my mouth. yes i watched it, but i also forced of my friends to watch it with me to achieve that end. i hope and pray we saw the directors cut... to think that there could be a version with more minutes of big eyebrowed lugs with baby oil glands rubbing their torsos just scares the hell out of me. so much of the film centers around this. i do applaud david decoteau for managing to lens this film in two days on a short budget, just as i applaud him for convincing people to pay him to make what is no doubt a celluloid version of some of his fetishes. but it isn a good film. the original shop of horrors was shot in the same length of time for a comparitive amount of money (considering inflation) and was an utter gem. it not an excuse for how bad this baby is. spoilers ahead... it not even worth picking apart the plot holes or cliched know it all hero characters... the pacing of the film... is insane... nothing... is interesting for the length of time decoteau dedicates to the pectoral self massaging. no matter what your alignment or sex... rubbing just cannot sustain that kind of screen time. the acting is cheese... but not overly amatuer... i seen a lot worse in better films... but somehow it the semi competent delivery of some of the worst lines you ever hear in a film, that really grates. rent this if, like me, you have a fascination with the worst of the worst, and only if you going to watch it with a group of people who are prepared to work to get through it. this is no ha ha ha the set wobbled affair. it an endurance test you probably want to skip. i sure there is worse... but i wouldn be surprised if it has decoteau name on it.\nI tell you what happened, some people with money thought it would be nice to ruin one of the best shows that was on TV. Did we really need a big screen re-make? Did they ask the fans? I wonder how all the fans would feel if they did a remake of \"Rocky Horror Picture Show\" with actors like Ashton Krutcher, Steve Martin, Britney Spears, and Kiefer Southerland, took out all the music, and made it a drama. Do you think they would like that! This movie does not have the same feel to it that the original had. Sure the original was a bit corny at times, but Bo and Luke were always nice, they got into trouble because they were always set up to get into trouble, and their main objective was to help people that passed through town. None of that mattered to the people that made this film, they might have never even seen the original show all the way through. My big question is, what will they ruin next?\nI tell you what happened, some people with money thought it would be nice to ruin one of the best shows that was on TV. Did we really need a big screen re-make? Did they ask the fans? I wonder how all the fans would feel if they did a remake of \"Rocky Horror Picture Show\" with actors like Ashton Krutcher, Steve Martin, Britney Spears, and Kiefer Southerland, took out all the music, and made it a drama. Do you think they would like that! This movie does not have the same feel to it that the original had. Sure the original was a bit corny at times, but Bo and Luke were always nice, they got into trouble because they were always set up to get into trouble, and their main objective was to help people that passed through town. None of that mattered to the people that made this film, they might have never even seen the original show all the way through. My big question is, what will they ruin next?\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T16:49:04.724062Z",
          "iopub.execute_input": "2024-03-05T16:49:04.724824Z",
          "iopub.status.idle": "2024-03-05T16:49:04.730263Z",
          "shell.execute_reply.started": "2024-03-05T16:49:04.724788Z",
          "shell.execute_reply": "2024-03-05T16:49:04.729433Z"
        },
        "trusted": true,
        "id": "66oshNfAEKsl",
        "outputId": "8fbc014f-197b-402d-cdc2-f8c9353f0fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'text': 'I have to start by telling you how I came across this movie.It was winter time in Alaska around the year .A friend of mine from Australia was staying with me and my girl friend in a shoe box of an apartment.Winters in Alaska can be a bit brutal and most people stay indoors,drink heavily and watch anything that comes on the television.I had found this movie outside of a thrift store laying in a snowbank and right away new it was a treasure.It is quite possibly the best worst movie ever.We spent the next two weeks watching this movie and drinking like fish.We watched it so many times in fact that we would sometimes turn the television on its side or upside down for a more full filling effect.It is a true gem.The laughs will come nonstop and the memories last forever.If you see this movie for rent in a video store,steal it.You won regret it!', 'label': 1}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It seems fine. Now it's time to create a HF dataset since to utilize LLM models with HF it's required"
      ],
      "metadata": {
        "id": "WrFdhWWSfxrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_df_cleaned.rename(columns={'sentiment': 'label'}, inplace=True) # the transformer model require that the Y is called label\n",
        "\n",
        "\n",
        "dataset = Dataset.from_pandas(train_df_cleaned) # load Dataset from Pandas DataFrame\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.2) # I split the dataset in two so that I can use an unseen part as evaluation data\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "print(dataset[\"train\"][100])\n",
        "print(dataset[\"test\"][10])"
      ],
      "metadata": {
        "id": "bGs-QiyOYd8o",
        "outputId": "e3b03594-0f73-4195-ffa7-d3f834a323e8",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:48:12.455155Z",
          "iopub.execute_input": "2024-03-05T16:48:12.456013Z",
          "iopub.status.idle": "2024-03-05T16:48:12.564121Z",
          "shell.execute_reply.started": "2024-03-05T16:48:12.455979Z",
          "shell.execute_reply": "2024-03-05T16:48:12.563194Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n})\n{'text': 'In this movie, Chvez supporters (either venezuelan and not-venezuelan) just lie about a dramatic situation in our country. They did not say that the conflict started because of Chvez announcement firing a lot of PDVSA best workers just for political issues. They did not say anything about more than TV interruptions transmitted by Chvez during only days in \"CADENA NACIONAL\" (a kind of confiscation o private TV signals). Each one with about minutes of duration. They did not tell us anything about The quiting announcement made by General en Jefe Lucas Rincon Romero, Inspector General of the army forces, who is a traditional supporter of Chvez. Even now, in despite of his announcement, he is the Ministro de Interior y Justicia. After Chvez return he occuped the Charge of Ministro del Defensa (equals to Defense Secretary in US). They did not say anything about Chvez orders about shooting against a pacifical people concentration who was claiming for elections. They did not say anything about the people in this concentration that were killed by Chvez Supporters (either civilians and Military official forces). They present some facts in a wrong order, in order to lie. They did not say anything about venezuelan civilian society thats are even now claiming for an elections in order to solve the crisis and Chvez actions in order to avoid the elections. That why i tell you.... This movie is just a lot of lies or a big lie.', 'label': 0}\n{'text': 'This movie has not aged well. Maybe it just the impact and artful characterization, acting, and directing that we seen with The Sopranos, but I just viewed Prizzi Honor for the first time, on DVD, alone. The experience of watching it with an audience years ago must have been quite different, but I have to say, I was just appalled at the ending. Not just the violence of it, but the mere idea that somehow this would be a satisfying ending. I enjoy a good shocker, but this seemed so out of character... Also, when was this move supposed to be set? The cars all seemed like they were from theand yet the World Trade Center towers {completed inwere clearly visible in many cityscape scenes. Another way in which the film has aged poorly is the mere idea that a passenger could travel coast to coast with a knife on his person. Somehow, mid- audiences found this film charming and funny. Mid-eighties, meet the late oughts: only of you can live.', 'label': 0}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To finetune the llm I need to use a tokenized version of the prompt, and it needs to be in the same encoding way as it the model was pretrained"
      ],
      "metadata": {
        "id": "F2SKzAOXhosd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "7nMNSRYIYmMp",
        "outputId": "86389bdf-2718-445a-bfb7-922ca7b03b32",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:49:41.934596Z",
          "iopub.execute_input": "2024-03-05T16:49:41.934998Z",
          "iopub.status.idle": "2024-03-05T16:49:57.656367Z",
          "shell.execute_reply.started": "2024-03-05T16:49:41.934963Z",
          "shell.execute_reply": "2024-03-05T16:49:57.655509Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "b422379b1b844a3bb6e0c1d9723c5499",
            "239a19cf8adb4be285c75a90afc91f1a",
            "e8ee1fe4909e4a8b9f5994a52706b899",
            "060a2828e48149e8bee397835cbac197",
            "a74ab3badb004f54a9c81c4a93b3f2b9",
            "268e8828eba54b6781706d94dfd1df70"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b422379b1b844a3bb6e0c1d9723c5499"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "239a19cf8adb4be285c75a90afc91f1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8ee1fe4909e4a8b9f5994a52706b899"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "060a2828e48149e8bee397835cbac197"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/20 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a74ab3badb004f54a9c81c4a93b3f2b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/5 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "268e8828eba54b6781706d94dfd1df70"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"test_trainer\")"
      ],
      "metadata": {
        "id": "5tlfEP5D4_hP",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:50:12.820764Z",
          "iopub.execute_input": "2024-03-05T16:50:12.821133Z",
          "iopub.status.idle": "2024-03-05T16:50:12.828801Z",
          "shell.execute_reply.started": "2024-03-05T16:50:12.821101Z",
          "shell.execute_reply": "2024-03-05T16:50:12.828012Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "loHAs_VMQGs3",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:50:31.676052Z",
          "iopub.execute_input": "2024-03-05T16:50:31.676434Z",
          "iopub.status.idle": "2024-03-05T16:50:32.399088Z",
          "shell.execute_reply.started": "2024-03-05T16:50:31.676403Z",
          "shell.execute_reply": "2024-03-05T16:50:32.398175Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "4c0df3f4f3084e71910a8b384f5cf9b1"
          ]
        },
        "outputId": "6d006b63-4952-4059-d5af-a458c95ab76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c0df3f4f3084e71910a8b384f5cf9b1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First to check how well the model is working I train it on a small part of the dataset. This way I have a faster way to try it with differt parameters and prompt format. And also important to not waste computational resurces"
      ],
      "metadata": {
        "id": "YVzhIj3_EKsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "light_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "light_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "6370S1RaYoyP",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:51:00.513812Z",
          "iopub.execute_input": "2024-03-05T16:51:00.514682Z",
          "iopub.status.idle": "2024-03-05T16:51:00.539653Z",
          "shell.execute_reply.started": "2024-03-05T16:51:00.514646Z",
          "shell.execute_reply": "2024-03-05T16:51:00.538708Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 18\n",
        "epochs = 3\n",
        "lr = 2e-5"
      ],
      "metadata": {
        "id": "pposHfO124VL",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:51:05.179887Z",
          "iopub.execute_input": "2024-03-05T16:51:05.180257Z",
          "iopub.status.idle": "2024-03-05T16:51:05.184963Z",
          "shell.execute_reply.started": "2024-03-05T16:51:05.180226Z",
          "shell.execute_reply": "2024-03-05T16:51:05.183838Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1,\n",
        "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
        "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
      ],
      "metadata": {
        "id": "6xeF4K7v3EGC",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:51:14.786788Z",
          "iopub.execute_input": "2024-03-05T16:51:14.787165Z",
          "iopub.status.idle": "2024-03-05T16:51:14.792916Z",
          "shell.execute_reply.started": "2024-03-05T16:51:14.787135Z",
          "shell.execute_reply": "2024-03-05T16:51:14.791912Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging\n",
        "logging.set_verbosity_error() # to avoid warning messages"
      ],
      "metadata": {
        "id": "efPs11_JEKsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "trainer = Trainer(model, args, train_dataset=light_train_dataset, eval_dataset=light_eval_dataset,\n",
        "                  tokenizer=tokenizer, compute_metrics=compute_metrics)"
      ],
      "metadata": {
        "id": "X6Mh2KEI3Gie",
        "outputId": "915c71f8-1091-43c9-9737-8408e5022118",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:51:29.356493Z",
          "iopub.execute_input": "2024-03-05T16:51:29.357203Z",
          "iopub.status.idle": "2024-03-05T16:51:31.946823Z",
          "shell.execute_reply.started": "2024-03-05T16:51:29.357176Z",
          "shell.execute_reply": "2024-03-05T16:51:31.946026Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "b4593961bddd4c9fa4f301c97ea8619e"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4593961bddd4c9fa4f301c97ea8619e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train();"
      ],
      "metadata": {
        "id": "0dOPxjYa3Nvy",
        "outputId": "c6f9b26a-1e3b-4ba4-e738-f96a6c1fdcb3",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:53:49.682069Z",
          "iopub.execute_input": "2024-03-05T16:53:49.682981Z",
          "iopub.status.idle": "2024-03-05T16:55:42.489651Z",
          "shell.execute_reply.started": "2024-03-05T16:53:49.682937Z",
          "shell.execute_reply": "2024-03-05T16:55:42.488752Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [168/168 01:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.433561</td>\n      <td>0.870000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.299047</td>\n      <td>0.882000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.316340</td>\n      <td>0.876000</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### the evaluation result seems pretty good, I can try it on the test set and see if it maintains similar results"
      ],
      "metadata": {
        "id": "ECa1xUVW6rag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### And as explained previously it needs to be encoded also in the same way"
      ],
      "metadata": {
        "id": "-DQ4yM6NEKso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = Dataset.from_pandas(test_df_cleaned).map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "WpqQKkvv-w9s",
        "outputId": "919ab8f4-f4e6-43c3-d431-37bd52ca6b68",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:58:41.313219Z",
          "iopub.execute_input": "2024-03-05T16:58:41.313603Z",
          "iopub.status.idle": "2024-03-05T16:58:54.752040Z",
          "shell.execute_reply.started": "2024-03-05T16:58:41.313572Z",
          "shell.execute_reply": "2024-03-05T16:58:54.751066Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "fe567f857e71449ebe444977ccae210e"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/25 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe567f857e71449ebe444977ccae210e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform the prediction"
      ],
      "metadata": {
        "id": "3kRUaUkpEKso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_light = trainer.predict(test_ds).predictions\n",
        "preds_light"
      ],
      "metadata": {
        "id": "Dt3vA7Lm-Y96",
        "outputId": "9fd4bebc-1abb-4228-9936-1d7b661b0d34",
        "execution": {
          "iopub.status.busy": "2024-03-05T16:58:58.953809Z",
          "iopub.execute_input": "2024-03-05T16:58:58.954562Z",
          "iopub.status.idle": "2024-03-05T17:03:05.176761Z",
          "shell.execute_reply.started": "2024-03-05T16:58:58.954500Z",
          "shell.execute_reply": "2024-03-05T17:03:05.175807Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([[ 1.2911031 , -1.564731  ],\n       [-0.666106  ,  0.55655736],\n       [ 1.3999647 , -1.8256439 ],\n       ...,\n       [ 1.4486115 , -1.8008753 ],\n       [-1.2210609 ,  1.176431  ],\n       [-1.0448589 ,  0.9568001 ]], dtype=float32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### And I need to extract the predictions from the 2D array"
      ],
      "metadata": {
        "id": "Chfh47waEKsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_light = np.argmax(preds_light, axis=-1)\n",
        "preds_light"
      ],
      "metadata": {
        "id": "ZlSjaxs1HowB",
        "outputId": "ab6d373b-7a01-43f9-9f77-126bc50cb860",
        "execution": {
          "iopub.status.busy": "2024-03-05T17:03:13.048861Z",
          "iopub.execute_input": "2024-03-05T17:03:13.049255Z",
          "iopub.status.idle": "2024-03-05T17:03:13.056252Z",
          "shell.execute_reply.started": "2024-03-05T17:03:13.049221Z",
          "shell.execute_reply": "2024-03-05T17:03:13.055283Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([0, 1, 0, ..., 0, 1, 1])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T17:49:26.549473Z",
          "iopub.execute_input": "2024-03-05T17:49:26.549902Z",
          "iopub.status.idle": "2024-03-05T17:49:26.555496Z",
          "shell.execute_reply.started": "2024-03-05T17:49:26.549864Z",
          "shell.execute_reply": "2024-03-05T17:49:26.554175Z"
        },
        "trusted": true,
        "id": "YQn3Uk7iEKsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_real = test_df_cleaned['sentiment']\n",
        "y_real = y_real.to_list()\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_real, preds_light)\n",
        "print('Accuracy: %f' % accuracy)"
      ],
      "metadata": {
        "id": "thHaizKrCqM3",
        "outputId": "0610e13b-d6b2-45d2-d367-25f6edd4df5d",
        "execution": {
          "iopub.status.busy": "2024-03-05T17:59:01.796707Z",
          "iopub.execute_input": "2024-03-05T17:59:01.797351Z",
          "iopub.status.idle": "2024-03-05T17:59:01.814554Z",
          "shell.execute_reply.started": "2024-03-05T17:59:01.797315Z",
          "shell.execute_reply": "2024-03-05T17:59:01.813688Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.874560\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I have an accuracy of circa 87% that is pretty good, now I can perform the finetuning on the full dataset"
      ],
      "metadata": {
        "id": "fkFnB9E_9HZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs_full = 18\n",
        "epochs_full = 3\n",
        "lr_full = 2e-5"
      ],
      "metadata": {
        "id": "me66dy349G9a",
        "execution": {
          "iopub.status.busy": "2024-03-05T17:13:48.937154Z",
          "iopub.execute_input": "2024-03-05T17:13:48.937906Z",
          "iopub.status.idle": "2024-03-05T17:13:48.942345Z",
          "shell.execute_reply.started": "2024-03-05T17:13:48.937864Z",
          "shell.execute_reply": "2024-03-05T17:13:48.941283Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args_full = TrainingArguments('outputs', learning_rate=lr_full, warmup_ratio=0.1,\n",
        "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs_full, per_device_eval_batch_size=bs_full*2,\n",
        "    num_train_epochs=epochs_full, weight_decay=0.01, report_to='none')"
      ],
      "metadata": {
        "id": "RBRvn2349hTk",
        "execution": {
          "iopub.status.busy": "2024-03-05T17:13:53.124229Z",
          "iopub.execute_input": "2024-03-05T17:13:53.124909Z",
          "iopub.status.idle": "2024-03-05T17:13:53.130012Z",
          "shell.execute_reply.started": "2024-03-05T17:13:53.124871Z",
          "shell.execute_reply": "2024-03-05T17:13:53.129078Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_full = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "trainer_full = Trainer(model_full, args_full, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"test\"],\n",
        "                  tokenizer=tokenizer, compute_metrics=compute_metrics)"
      ],
      "metadata": {
        "outputId": "f62f8d22-9fe3-4269-e68d-1d184b2bf7a6",
        "id": "sGRBrjo19kKL",
        "execution": {
          "iopub.status.busy": "2024-03-05T17:14:00.094183Z",
          "iopub.execute_input": "2024-03-05T17:14:00.095146Z",
          "iopub.status.idle": "2024-03-05T17:14:01.429665Z",
          "shell.execute_reply.started": "2024-03-05T17:14:00.095104Z",
          "shell.execute_reply": "2024-03-05T17:14:01.428683Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_full.train();"
      ],
      "metadata": {
        "outputId": "789bf956-d44d-45dc-997b-9a09593e5736",
        "id": "8DQD1-Cf9n3s",
        "execution": {
          "iopub.status.busy": "2024-03-05T17:14:07.343093Z",
          "iopub.execute_input": "2024-03-05T17:14:07.343755Z",
          "iopub.status.idle": "2024-03-05T17:44:24.250611Z",
          "shell.execute_reply.started": "2024-03-05T17:14:07.343725Z",
          "shell.execute_reply": "2024-03-05T17:44:24.249602Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='3336' max='3336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3336/3336 30:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.241900</td>\n      <td>0.227941</td>\n      <td>0.911600</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.165400</td>\n      <td>0.233922</td>\n      <td>0.921000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.096900</td>\n      <td>0.310880</td>\n      <td>0.922000</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I can see that now I have a better accuracy, so I can perform again the prediction, this time with the new model and submit it"
      ],
      "metadata": {
        "id": "FlTtoRiMEKsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_full = trainer_full.predict(test_ds).predictions\n",
        "preds_full = np.argmax(preds_full, axis=-1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T17:45:20.114568Z",
          "iopub.execute_input": "2024-03-05T17:45:20.115501Z",
          "iopub.status.idle": "2024-03-05T17:49:26.547295Z",
          "shell.execute_reply.started": "2024-03-05T17:45:20.115465Z",
          "shell.execute_reply": "2024-03-05T17:49:26.546362Z"
        },
        "trusted": true,
        "id": "tK1_z08sEKsx",
        "outputId": "dfbfbef2-8daf-4763-f6a4-f3682b019c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_real = test_df_cleaned['sentiment']\n",
        "y_real = y_real.to_list()\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_real, preds_full)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_real, preds_full)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_real, preds_full)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_real, preds_full)\n",
        "print('F1 score: %f' % f1)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T17:53:41.222622Z",
          "iopub.execute_input": "2024-03-05T17:53:41.223290Z",
          "iopub.status.idle": "2024-03-05T17:53:41.318444Z",
          "shell.execute_reply.started": "2024-03-05T17:53:41.223259Z",
          "shell.execute_reply": "2024-03-05T17:53:41.317503Z"
        },
        "trusted": true,
        "id": "aeslfecOEKsx",
        "outputId": "02c5abc7-8ce4-431b-d3a2-6b0bd423fc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.929520\nPrecision: 0.930484\nRecall: 0.928400\nF1 score: 0.929441\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I have an accuracy of 93% on the test set"
      ],
      "metadata": {
        "id": "4L1XEpynEyof"
      }
    }
  ]
}